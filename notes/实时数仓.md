# 1. 维度表设计
- 离线数仓中，普通维度表通过主维表和相关维表做关联查询生成，对应的业务数据是通过每日一次<font color='skyblue'>全量同步</font>到 `HDFS` 中，只需每日做一次全量数据的关联查询
- 实时数仓采集的是所有表的变化数据，一旦主维表或相关维表中的某张数据表数据发生了变化，就需要和其他表的历史数据做关联
- 获取历史数据的方法：
	- 在某张与维度表相关的业务数据表发生变化时，执行一次 `maxwell-bootstrap` 命令，将相关业务维度表的数据导入 `Kafka`
		- 缺点：
			- `Kafka` 会存储冗余数据
			-  需要组件来执行 `maxwell-bootstrap` 命令
			- 数据进入流中的时间不同，可能会出现 `join` 不到的情况，影响时效性
	- 维度表发生变化时去 `HBase` 中读取关联后的维表，筛选出受影响的数据，与变更后的数据进行关联再写入 `HBase`
		- 缺点：一条数据发生变化后，受到影响的数据可能会特别多，影响效率
	- 将分表导入 `HBase`，在 `HBase` 中进行关联
		- 缺点：`HBase` 的 `join` 性能很差，关联操作不在 `Stream` 的 `DAG` 图中，需要单独调度
	- 综上，对业务表做 `join` 形成维度表的方式不适用于实时数仓
- 在实时数仓中，不再对业务数据库中的维度表进行合并，过滤掉一些不需要的字段后，将维度数据写入 `HBase` 的维度表中
- 实时数仓强调实时性，不保存历史事实数据，但需要考虑历史维度数据，字典表数据量小，选择将其维度字段退化到事实表中
# 2. 项目架构
- 创建一个父工程，在 `properties` 中声明框架版本号，不需要打包的依赖需要添加 `<scope> provided </scope>` 标签
- 在父工程 `pom` 文件中通过 `<dependencyManagement>` 标签集中管理项目中所有依赖项的版本，而不是真正地声明这些依赖，确保所有模块都使用相同的依赖版本，从而避免了版本冲突
- 子模块添加已被管理的依赖时不需要声明版本，因为它已经在父工程定义了
- 在父工程下创建四个 module：
	- common：引入公共第三方依赖，编写工具类
	- 在 dim, dwd, dws 中引入 common 模块
- common 模块的 package 结构：
	- base：所有业务代码的基类
	- bean：所有的实体类
	- constant
	- function
	- util
# 3. ODS
- 需要对维度表相关的数据做一次全量同步
# 4. DIM
- `DIM` 层的数据存储在 `HBase` 表中
## 4.1 基类设计
- `Flink Job` 的处理流程：
	- 初始化流处理环境，配置检查点，从 `Kafka` 读取目标主题数据
	- 执行处理逻辑
	- `execute`
- 将第一步和第三步交给基类完成，定义实现处理逻辑的抽象方法交给子类重写
```java
public void start(int port, int parallelism, String ckAndGroupId, String topic) {  
    // 1. 环境准备  
    // 1.1 设置操作 Hadoop 的用户
    System.setProperty("HADOOP_USER_NAME", "ranran");  
  
    // 1.2 获取流处理环境，并指定本地测试时启动 WebUI 所绑定的端口  
    Configuration conf = new Configuration();  
    conf.setInteger("rest.port", port);  
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(conf);  
  
    // 1.3 设置并行度  
    env.setParallelism(parallelism);  
  
    // 1.4 状态后端及检查点相关配置  
    // 1.4.1 设置状态后端  
    env.setStateBackend(new HashMapStateBackend());  
  
    // 1.4.2 开启 checkpoint    
    env.enableCheckpointing(5000);  
    // 1.4.3 设置 checkpoint 模式: 精准一次 
    env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);  
    
    // 1.4.4 checkpoint 存储  
    env.getCheckpointConfig().setCheckpointStorage("hdfs://hadoop102:8020/stream/" + ckAndGroupId);  
    // 1.4.5 checkpoint 并发数  
    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);  
    // 1.4.6 checkpoint 之间的最小间隔  
    env.getCheckpointConfig().setMinPauseBetweenCheckpoints(5000);  
    // 1.4.7 checkpoint 的超时时间  
    env.getCheckpointConfig().setCheckpointTimeout(10000);  
    // 1.4.8 job 取消时 checkpoint 保留策略  
    env.getCheckpointConfig().setExternalizedCheckpointCleanup(RETAIN_ON_CANCELLATION);  
  
    // 1.5 从 Kafka 目标主题读取数据，封装为流  
    KafkaSource<String> source = FlinkSourceUtil.getKafkaSource(ckAndGroupId, topic);  
  
    DataStreamSource<String> stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), "kafka_source");  
  
    // 2. 执行具体的处理逻辑  
    handle(env, stream);  
  
    // 3. 执行 Job    
    try {  
        env.execute();  
    } catch (Exception e) {  
        e.printStackTrace();  
    }  
}
```
- 在 `Constant` 类中定义配置信息
- 在 `FlinkSourceUtil` 中定义和 `Kafka` 交互的 `Source`
```java
public static KafkaSource<String> getKafkaSource(String groupId,  
                                                 String topic) {  
    return KafkaSource.<String>builder()  
            .setBootstrapServers(Constant.KAFKA_BROKERS)  
            .setGroupId(groupId)  
            .setTopics(topic)  
            // 当Flink中更新数据，产生+U的操作时，会向Kafka中发送一条空数据，此时不能通过SimpleStringSchema进行反序列化，选择DeserializationSchema
            .setValueOnlyDeserializer(new DeserializationSchema<String>() {  
                @Override  
                public String deserialize(byte[] bytes) throws IOException {  
                    if (bytes != null && bytes.length != 0) {  
                        return new String(bytes);  
                    }  
                    return null;  
                }  
  
                @Override  
                public boolean isEndOfStream(String s) {  
                    return false;  
                }  
  
                @Override  
                public TypeInformation<String> getProducedType() {  
                    return Types.STRING;  
                }  
            })  
            .setStartingOffsets(OffsetsInitializer.earliest())  
            .build();  
}
```
## 4.2 Flink CDC(Change Data Capture)
- `DIM` 层需要动态筛选维度表数据，通过表名过滤掉不是维度表的数据，维度表的相关信息需要先在 `MySql` 中创建一张配置表：
```sql
+------------+----------+-----------+------------+------------+
|source_table|sink_table|sink_family|sink_columns|sink_row_key|

source_table: 业务数据库中表名
sink_table: HBase中需要创建的表的表名
sink_family: 写入HBase时的列族名，统一取为info
sink_columns: 写入的字段名
sink_row_key: 写入时的row_key
```
- 在 `/etc/my.cnf` 中增加 `binlog` 的配置: `binlog-do-db=${db_name}`
- 数据格式
- 修改维度表数据
	- 修改前的数据在 `before` 中，修改后的数据在 `after` 中，操作对应类型为 `op: u`
```json
{
    "before": {
		// 修改前的所有字段
    },
    "after": {
        // 修改后的所有字段
    },
    "source": {
        "version": "1.9.7.Final",
        "connector": "mysql",
        "name": "mysql_binlog_source",
        "ts_ms": 1702102182000,
        "snapshot": "false",
        "db": "gmall",
        "sequence": null,
        "table": "activity_info",
        "server_id": 1,
        "gtid": null,
        "file": "mysql-bin.000126",
        "pos": 401,
        "row": 0,
        "thread": 8,
        "query": null
    },
    "op": "u",
    "ts_ms": 1702102182622,
    "transaction": null
}
```
- 维度表新增一条数据
	- 新增数据保存在 `after` 中，对应的操作类型是 `op: c`
```json
{
    "before": null,
    "after": {
        // 新增的数据
    },
    "source": {...},
    "op": "c",
    "ts_ms": 1702102239959,
    "transaction": null
}
```
- 维度表删除一条数据
	- 被删除的数据在 `before` 中，对应的操作类型是 `op: d`
```json
{

    "before": {
	    // 被删除的数据
    },
    "after": null,
    "source": {...},
    "op": "d",
    "ts_ms": 1702102247152,
    "transaction": null
}
```
- 查询数据
	- 数据保存在 after 中
```json
{
    "before": null,
    "after": {
        // 原始数据
    },
    "source": {...},
    "op": "r",
    "ts_ms": 1702102239959,
    "transaction": null
}
```
## 4.3 业务逻辑
### 4.3.1 过滤来自 Kafka 的数据
- `Kafka` 中的数据，可能有：不是 `json` 格式的数据，`maxwell` 的 `bootstrap` 命令产生的无用数据，需要进行过滤
- maxwell 通过 bootstrap 产生的数据格式：
```json
{
    "database": "db_name",
    "table": "tbl_name",
    "type": "bootstrap-start",
    "ts": 1654941947,
    "data": {}
}

{
    "database": "db_name",
    "table": "tbl_name",
    "type": "bootstrap-insert",
    "ts": 1654941951,
    "data": {
        // 插入数据存储在data中
    }
}

{
    "database": "gmall",
    "table": "activity_sku",
    "type": "bootstrap-complete",
    "ts": 1654941955,
    "data": {}
}
```

```java
stream.flatMap((FlatMapFunction<String, JSONObject>) (value, out) -> { 
    // 直接通过JSONObject.parseObject方法，将stream转换为jsonObj
    JSONObject jsonObj = JSONObject.parseObject(value);  
    // data对应json中的对象，需要通过getJSONObject()提取
    JSONObject data = jsonObj.getJSONObject("data");  
    String type = jsonObj.getString("type");  
    if (!("bootstrap-start".equals(type) || "bootstrap-complete".equals(type))) {  
        // 过滤掉空数据，发往下游
        if (data != null) {  
            out.collect(jsonObj);  
        }  
    }  
});
```
### 4.3.2 读取到 MySql 配置维度表信息，转换为实体类
```java
// 在FlinkSourceUtil中增加连接MySql的配置
// 通过传入Properties对象，可以设置连接mysqlURL中其他的配置
Properties prop = new Properties();  
prop.setProperty("useSSL", "false");  
prop.setProperty("allowPublicKeyRetrieval", "true");

MySqlSource.<String>builder()  
        .hostname(Constant.MYSQL_HOST)  
        .port(Constant.MYSQL_PORT)  
        .username(Constant.MYSQL_USER_NAME)  
        .password(Constant.MYSQL_PASSWORD)  
        .jdbcProperties(prop)  
        .startupOptions(StartupOptions.initial())  
        .databaseList(dbName)  
        .tableList(dbName + "." + tblName)  
        .deserializer(new JsonDebeziumDeserializationSchema())  
        .build();
```

```java
// fromSource()转换为stream后，通过map转换为bean：
/*
 * 转换时需要考虑op的类型：
 *     d: 从before中获取数据, 否则从after中获取数据
 */
 String op = jsonObj.getString("op");  
if ("d".equals(op)) { // 数据在before中  
    JSONObject before = jsonObj.getJSONObject("before");  
    // 封装bean  
    return new TableProcessDim(  
            before.getString("source_table"),  
            before.getString("sink_table"),  
            before.getString("sink_columns"),  
            before.getString("sink_family"),  
            before.getString("sink_row_key"),  
            // 配置表中没有op字段，在创建bean时传入
            op  
    );
else{...}
```
### 4.3.3 动态筛选维度表数据
- 将配置流作为广播流，连接来自 `Kafka` 的主流
```java
// 创建MapStateDescriptor，通过表名筛选维度表
MapStateDescriptor<String, TableProcessDim> dimDesc = new MapStateDescriptor<>("mapState", String.class, TableProcessDim.class);  // 广播配置流
BroadcastStream<TableProcessDim> broadcastStream = configStream.broadcast(dimMapStateDescriptor);
// 主流连接广播后的配置流
mainStream.connect(broadcastStream)
	// IN1, IN2, OUT
	// 主流中没有写出HBase相关的信息，需要同bean一同封装写出, 因此使用Tuple2
	.process(new BroadcastProcessFunction<JSONObject, TableProcessDim, Tuple2<JSONObject, TableProcessDim>>() {
		
		public void processElement(JSONObject value, ReadOnlyContext ctx, Collector<Tuple2<JSONObject, TableProcessDim>> out) throws Exception {
		// 处理主流数据，需要通过表名，判断能否从广播状态中获取到bean
		// 获取不到则说明不是维度表
		// maxwell数据的table字段中获取到表名
		String tblName = value.getString("table");
		// 去状态中获取对应的bean
		TableProcessDim bean = ctx.getBroadcastState(dimDesc).get(tblName)
		// 如果bean为null，①：不是维度表，②：配置流发送的速度小于主流，导致没有获取到
		// 针对②的情况，通过open方法，在刚进入process方法时，首先去mysql读取配置表的全部信息，封装成bean放在map中
		// 当第一次没有获取到bean时，就去map中get，如果还为空，说明对应①的情况
		if (tableProcessDim == null) {  
		    tableProcessDim = map.get(tblName);  
			}  
		if (tableProcessDim != null) {  
		    // 封装成Tuple写出  
		    out.collect(Tuple2.of(value, tableProcessDim));  
			}
		}
	}

		public void open(Configuration parameters) throws Exception {
			Connection conn = JdbcUtil.getConnection();  
			List<TableProcessDim> processDims = JdbcUtil.querySql(...);  
			// 遍历添加
			for (TableProcessDim processDim : processDims) {  
			    processDim.setOp("r");  
			    map.put(processDim.getSourceTable(), processDim);  
			}  
			JdbcUtil.close(conn);
		}

		public void processElement(JSONObject value, ReadOnlyContext ctx, Collector<Tuple2<JSONObject, TableProcessDim>> out) throws Exception {
			// 针对配置流数据，需要根据操作类型来及时修改维度表配置
			// 如果是d类型op，需要从state和map中移除数据
			BroadcastState<String, TableProcessDim> broadcastState = ctx.getBroadcastState(dimDesc);
			String op = value.getString("op")











	})
```