# 1 维度表设计
- 离线数仓中，普通维度表通过主维表和相关维表做关联查询生成，对应的业务数据是通过每日一次<font color='skyblue'>全量同步</font>到 `HDFS` 中，只需每日做一次全量数据的关联查询
- 实时数仓采集的是所有表的变化数据，一旦主维表或相关维表中的某张数据表数据发生了变化，就需要和其他表的历史数据做关联
- 获取历史数据的方法：
	- 在某张与维度表相关的业务数据表发生变化时，执行一次 `maxwell-bootstrap` 命令，将相关业务维度表的数据导入 `Kafka`
		- 缺点：
			- `Kafka` 会存储冗余数据
			-  需要组件来执行 `maxwell-bootstrap` 命令
			- 数据进入流中的时间不同，可能会出现 `join` 不到的情况，影响时效性
	- 维度表发生变化时去 `HBase` 中读取关联后的维表，筛选出受影响的数据，与变更后的数据进行关联再写入 `HBase`
		- 缺点：一条数据发生变化后，受到影响的数据可能会特别多，影响效率
	- 将分表导入 `HBase`，在 `HBase` 中进行关联
		- 缺点：`HBase` 的 `join` 性能很差，关联操作不在 `Stream` 的 `DAG` 图中，需要单独调度
	- 综上，对业务表做 `join` 形成维度表的方式不适用于实时数仓
- 在实时数仓中，不再对业务数据库中的维度表进行合并，过滤掉一些不需要的字段后，将维度数据写入 `HBase` 的维度表中
- 实时数仓强调实时性，不保存历史事实数据，但需要考虑历史维度数据，字典表数据量小，选择将其维度字段退化到事实表中
# 2 项目架构
- 创建一个父工程，在 `properties` 中声明框架版本号，不需要打包的依赖需要添加 `<scope> provided </scope>` 标签
- 在父工程 `pom` 文件中通过 `<dependencyManagement>` 标签集中管理项目中所有依赖项的版本，而不是真正地声明这些依赖，确保所有模块都使用相同的依赖版本，从而避免了版本冲突
- 子模块添加已被管理的依赖时不需要声明版本，因为它已经在父工程定义了
- 在父工程下创建四个 module：
	- common：引入公共第三方依赖，编写工具类
	- 在 dim, dwd, dws 中引入 common 模块
- common 模块的 package 结构：
	- base：所有业务代码的基类
	- bean：所有的实体类
	- constant
	- function
	- util
# 3 ODS
- 需要对维度表相关的数据做一次全量同步
# 4 DIM
- `DIM` 层的数据存储在 `HBase` 表中
## 4.1 基类设计
- `Flink Job` 的处理流程：
	- 初始化流处理环境，配置检查点，从 `Kafka` 读取目标主题数据
	- 执行处理逻辑
	- `execute`
- 将第一步和第三步交给基类完成，定义实现处理逻辑的抽象方法交给子类重写
```java
public void start(int port, int parallelism, String ckAndGroupId, String topic) {  
    // 1. 环境准备  
    // 1.1 设置操作 Hadoop 的用户
    System.setProperty("HADOOP_USER_NAME", "ranran");  
  
    // 1.2 获取流处理环境，并指定本地测试时启动 WebUI 所绑定的端口  
    Configuration conf = new Configuration();  
    conf.setInteger("rest.port", port);  
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(conf);  
  
    // 1.3 设置并行度  
    env.setParallelism(parallelism);  
  
    // 1.4 状态后端及检查点相关配置  
    // 1.4.1 设置状态后端  
    env.setStateBackend(new HashMapStateBackend());  
  
    // 1.4.2 开启 checkpoint    
    env.enableCheckpointing(5000);  
    // 1.4.3 设置 checkpoint 模式: 精准一次 
    env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);  
    
    // 1.4.4 checkpoint 存储  
    env.getCheckpointConfig().setCheckpointStorage("hdfs://hadoop102:8020/stream/" + ckAndGroupId);  
    // 1.4.5 checkpoint 并发数  
    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);  
    // 1.4.6 checkpoint 之间的最小间隔  
    env.getCheckpointConfig().setMinPauseBetweenCheckpoints(5000);  
    // 1.4.7 checkpoint 的超时时间  
    env.getCheckpointConfig().setCheckpointTimeout(10000);  
    // 1.4.8 job 取消时 checkpoint 保留策略  
    env.getCheckpointConfig().setExternalizedCheckpointCleanup(RETAIN_ON_CANCELLATION);  
  
    // 1.5 从 Kafka 目标主题读取数据，封装为流  
    KafkaSource<String> source = FlinkSourceUtil.getKafkaSource(ckAndGroupId, topic);  
  
    DataStreamSource<String> stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), "kafka_source");  
  
    // 2. 执行具体的处理逻辑  
    handle(env, stream);  
  
    // 3. 执行 Job    
    try {  
        env.execute();  
    } catch (Exception e) {  
        e.printStackTrace();  
    }  
}
```
- 在 `Constant` 类中定义配置信息
- 在 `FlinkSourceUtil` 中定义和 `Kafka` 交互的 `Source`
```java
public static KafkaSource<String> getKafkaSource(String groupId,  
                                                 String topic) {  
    return KafkaSource.<String>builder()  
            .setBootstrapServers(Constant.KAFKA_BROKERS)  
            .setGroupId(groupId)  
            .setTopics(topic)  
            // 当Flink中更新数据，产生+U的操作时，会向Kafka中发送一条空数据，此时不能通过SimpleStringSchema进行反序列化，选择DeserializationSchema
            .setValueOnlyDeserializer(new DeserializationSchema<String>() {  
                @Override  
                public String deserialize(byte[] bytes) throws IOException {  
                    if (bytes != null && bytes.length != 0) {  
                        return new String(bytes);  
                    }  
                    return null;  
                }  
  
                @Override  
                public boolean isEndOfStream(String s) {  
                    return false;  
                }  
  
                @Override  
                public TypeInformation<String> getProducedType() {  
                    return Types.STRING;  
                }  
            })  
            .setStartingOffsets(OffsetsInitializer.earliest())  
            .build();  
}
```
## 4.2 Flink CDC(Change Data Capture)
- `DIM` 层需要动态筛选维度表数据，通过表名过滤掉不是维度表的数据，维度表的相关信息需要先在 `MySql` 中创建一张配置表：
```sql
+------------+----------+-----------+------------+------------+
|source_table|sink_table|sink_family|sink_columns|sink_row_key|

source_table: 业务数据库中表名
sink_table: HBase中需要创建的表的表名
sink_family: 写入HBase时的列族名，统一取为info
sink_columns: 写入的字段名
sink_row_key: 写入时的row_key
```
- 在 `/etc/my.cnf` 中增加 `binlog` 的配置: `binlog-do-db=${db_name}`
- 数据格式
- 修改维度表数据
	- 修改前的数据在 `before` 中，修改后的数据在 `after` 中，操作对应类型为 `op: u`
```json
{
    "before": {
		// 修改前的所有字段
    },
    "after": {
        // 修改后的所有字段
    },
    "source": {
        "version": "1.9.7.Final",
        "connector": "mysql",
        "name": "mysql_binlog_source",
        "ts_ms": 1702102182000,
        "snapshot": "false",
        "db": "gmall",
        "sequence": null,
        "table": "activity_info",
        "server_id": 1,
        "gtid": null,
        "file": "mysql-bin.000126",
        "pos": 401,
        "row": 0,
        "thread": 8,
        "query": null
    },
    "op": "u",
    "ts_ms": 1702102182622,
    "transaction": null
}
```
- 维度表新增一条数据
	- 新增数据保存在 `after` 中，对应的操作类型是 `op: c`
```json
{
    "before": null,
    "after": {
        // 新增的数据
    },
    "source": {...},
    "op": "c",
    "ts_ms": 1702102239959,
    "transaction": null
}
```
- 维度表删除一条数据
	- 被删除的数据在 `before` 中，对应的操作类型是 `op: d`
```json
{

    "before": {
	    // 被删除的数据
    },
    "after": null,
    "source": {...},
    "op": "d",
    "ts_ms": 1702102247152,
    "transaction": null
}
```
- 查询数据
	- 数据保存在 after 中
```json
{
    "before": null,
    "after": {
        // 原始数据
    },
    "source": {...},
    "op": "r",
    "ts_ms": 1702102239959,
    "transaction": null
}
```
## 4.3 业务逻辑
### 4.3.1 过滤来自 Kafka 的数据
- `Kafka` 中的数据，可能有：不是 `json` 格式的数据，`maxwell` 的 `bootstrap` 命令产生的无用数据，需要进行过滤
- maxwell 通过 bootstrap 产生的数据格式：
```json
{
    "database": "db_name",
    "table": "tbl_name",
    "type": "bootstrap-start",
    "ts": 1654941947,
    "data": {}
}

{
    "database": "db_name",
    "table": "tbl_name",
    "type": "bootstrap-insert",
    "ts": 1654941951,
    "data": {
        // 插入数据存储在data中
    }
}

{
    "database": "gmall",
    "table": "activity_sku",
    "type": "bootstrap-complete",
    "ts": 1654941955,
    "data": {}
}
```

```java
stream.flatMap((FlatMapFunction<String, JSONObject>) (value, out) -> { 
    // 直接通过JSONObject.parseObject方法，将stream转换为jsonObj
    JSONObject jsonObj = JSONObject.parseObject(value);  
    // data对应json中的对象，需要通过getJSONObject()提取
    JSONObject data = jsonObj.getJSONObject("data");  
    String type = jsonObj.getString("type");  
    if (!("bootstrap-start".equals(type) || "bootstrap-complete".equals(type))) {  
        // 过滤掉空数据，发往下游
        if (data != null) {  
            out.collect(jsonObj);  
        }  
    }  
});
```
### 4.3.2 读取到 MySql 配置维度表信息，转换为实体类
```java
// 在FlinkSourceUtil中增加连接MySql的配置
// 通过传入Properties对象，可以设置连接mysqlURL中其他的配置
Properties prop = new Properties();  
prop.setProperty("useSSL", "false");  
prop.setProperty("allowPublicKeyRetrieval", "true");

MySqlSource.<String>builder()  
        .hostname(Constant.MYSQL_HOST)  
        .port(Constant.MYSQL_PORT)  
        .username(Constant.MYSQL_USER_NAME)  
        .password(Constant.MYSQL_PASSWORD)  
        .jdbcProperties(prop)  
        .startupOptions(StartupOptions.initial())  
        .databaseList(dbName)  
        .tableList(dbName + "." + tblName)  
        .deserializer(new JsonDebeziumDeserializationSchema())  
        .build();
```

```java
// fromSource()转换为stream后，通过map转换为bean：
/*
 * 转换时需要考虑op的类型：
 *     d: 从before中获取数据, 否则从after中获取数据
 */
 String op = jsonObj.getString("op");  
if ("d".equals(op)) { // 数据在before中  
    JSONObject before = jsonObj.getJSONObject("before");  
    // 封装bean  
    return new TableProcessDim(  
            before.getString("source_table"),  
            before.getString("sink_table"),  
            before.getString("sink_columns"),  
            before.getString("sink_family"),  
            before.getString("sink_row_key"),  
            // 配置表中没有op字段，在创建bean时传入
            op  
    );
else{...}
```
### 4.3.3 通过 bean 在 HBase 中建表
#### 4.3.3.1 创建连接 HBase 的工具类
```java
public static Connection getHBaseConnection() throws IOException {  
  
    Configuration conf = new Configuration();  
    // 指定zk的地址
    conf.set("hbase.zookeeper.quorum", Constant.HBASE_ZOOKEEPER_QUORUM);  
    return ConnectionFactory.createConnection(conf);  
}

public static void createTbl(Connection conn, String namespace, String tblName, String... families) throws IOException {  
	// HBase的DDL语句需要先获得一个Admin对象
    Admin admin = conn.getAdmin();  
  
    TableName tableName = TableName.valueOf(namespace, tblName);  
    if (admin.tableExists(tableName)) {  
        System.out.println("表已存在!");  
        return;  
    }  
  
    if (families == null || families.length == 0) {  
        System.out.println("需要列族才能创建表");  
        return;  
    }  

	// createTable()需要传入一个TableDescriptorBuilder调用build()生成的TableDescriptor
	// TableDescriptorBuilder.setColumnFamily()需要传入ColumnFamilyDescriptorBuilder.build()生成的TableDescriptor

    TableDescriptorBuilder descriptorBuilder = TableDescriptorBuilder  
            .newBuilder(tableName);  
  
    for (String family : families) {  
        ColumnFamilyDescriptorBuilder familyDescriptorBuilder = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(family));  
        descriptorBuilder.setColumnFamily(familyDescriptorBuilder.build());  
    }  
    admin.createTable(descriptorBuilder.build()); 
    admin.close();
}

public static void dropTbl(Connection conn, String namespace, String tblName) throws IOException {  
    Admin admin = conn.getAdmin();  
    TableName name = TableName.valueOf(namespace, tblName);  

	// 删除时需要先disable
    try {  
        admin.disableTable(name);  
        admin.deleteTable(name);  
    } catch (IOException e) {  
        throw new RuntimeException(e);  
    }  
    admin.close();  
}
```

```java
// 根据op类型决定建表操作
public TableProcessDim map(TableProcessDim value) throws Exception {  
    if ("d".equals(value.getOp())) {  
        dropTbl(value);  
    } else if ("c".equals(value.getOp()) || "r".equals(value.getOp())) {  
        createTbl(value);  
    } else {  
        // 更新需要先删除旧表, 再创建新表  
        dropTbl(value);  
        createTbl(value);  
    }  
    return value;  
}
```
### 4.3.4 动态筛选维度表数据, 保存到 HBase
- 将配置流作为广播流，连接来自 `Kafka` 的主流
```java
// 创建MapStateDescriptor，通过表名筛选维度表
MapStateDescriptor<String, TableProcessDim> dimDesc = new MapStateDescriptor<>("mapState", String.class, TableProcessDim.class);  // 广播配置流
BroadcastStream<TableProcessDim> broadcastStream = configStream.broadcast(dimMapStateDescriptor);
// 主流连接广播后的配置流
mainStream.connect(broadcastStream)
	// IN1, IN2, OUT
	// 主流中没有写出HBase相关的信息，需要同bean一同封装写出, 因此使用Tuple2
	.process(new BroadcastProcessFunction<JSONObject, TableProcessDim, Tuple2<JSONObject, TableProcessDim>>() {
		
		public void processElement(JSONObject value, ReadOnlyContext ctx, Collector<Tuple2<JSONObject, TableProcessDim>> out) throws Exception {
		// 处理主流数据，需要通过表名，判断能否从广播状态中获取到bean
		// 获取不到则说明不是维度表
		// maxwell数据的table字段中获取到表名
		String tblName = value.getString("table");
		// 去状态中获取对应的bean
		TableProcessDim bean = ctx.getBroadcastState(dimDesc).get(tblName)
		// 如果bean为null，①：不是维度表，②：配置流发送的速度小于主流，导致没有获取到
		// 针对②的情况，通过open方法，在刚进入process方法时，首先去mysql读取配置表的全部信息，封装成bean放在map中
		// 当第一次没有获取到bean时，就去map中get，如果还为空，说明对应①的情况
		if (tableProcessDim == null) {  
		    tableProcessDim = map.get(tblName);  
			}  
		if (tableProcessDim != null) {  
		    // 封装成Tuple写出  
		    out.collect(Tuple2.of(value, tableProcessDim));  
			}
		}
	}

		public void open(Configuration parameters) throws Exception {
			Connection conn = JdbcUtil.getConnection();  
			List<TableProcessDim> processDims = JdbcUtil.querySql(...);  
			// 遍历添加
			for (TableProcessDim processDim : processDims) {  
			    processDim.setOp("r");  
			    map.put(processDim.getSourceTable(), processDim);  
			}  
			JdbcUtil.close(conn);
		}

		public void processElement(JSONObject value, ReadOnlyContext ctx, Collector<Tuple2<JSONObject, TableProcessDim>> out) throws Exception {
			// 针对配置流数据，需要根据操作类型来及时修改维度表配置
			// 如果是d类型op，需要从state和map中移除数据
			BroadcastState<String, TableProcessDim> broadcastState = ctx.getBroadcastState(dimDesc);
			if ("d".equals(value.getString("op"))) {
				// 通过维度表表名移除数据
				broadcastState.remove(value.getSourceTable());
				map.remove(value.getSourceTable());
			} else {
				// 添加到状态中
				broadcastState.put(value.getSourceTable(), value);	
			}
		}
	});
```
### 4.3.5 通过 JDBC 获取维度表配置数据
```java
// 通过DriverManager获取连接
public static Connection getConnection() throws SQLException {  
    return DriverManager.getConnection(Constant.MYSQL_URL,  
            Constant.MYSQL_USER_NAME,  
            Constant.MYSQL_PASSWORD);  
}

public static <T> List<T> querySql(Connection conn, String sql, Class<T> t, boolean transform) {  
    ArrayList<T> res = null;  
    try {  
        PreparedStatement ps = conn.prepareStatement(sql);  
        ResultSet resultSet = ps.executeQuery();  
		// 将查询到的一行数据写入bean中
        res = new ArrayList<>();  
        ResultSetMetaData metaData = resultSet.getMetaData();  
  
        while (resultSet.next()) { // hasNext() + next()
            T t1 = t.newInstance();  
            for (int i = 1; i <= metaData.getColumnCount(); i++) { 
	            // 从元数据获取列名 
                String colName = metaData.getColumnName(i);  
                if (transform) {  
	                // mysql中的数据是下划线形式，需要转换为bean中的驼峰形式
                    colName = CaseFormat.LOWER_UNDERSCORE.to(CaseFormat.LOWER_CAMEL, metaData.getColumnName(i));  
                }  
                String colValue = resultSet.getString(i);  
                // 通过反射给bean赋值
                BeanUtils.setProperty(t1, colName, colValue);  
            }  
            res.add(t1);  
        }  
        ps.close();  
    } catch (Exception e) {  
        throw new RuntimeException(e);  
    }  
    return res;  
}
```
### 4.3.5 筛选出 sink_columns 中需要的字段
```java
// 上一步传过来的Tuple2中，包含了bean，以及对应的维度数据，通过bean中的sink_columns得到需要的字段，从维度数据中移除后写出
public JSONObject map(Tuple2<JSONObject, TableProcessDim> value) throws Exception {  
    JSONObject jsonObject = value.f0;  
    TableProcessDim tp = value.f1;  
    JSONObject data = jsonObject.getJSONObject("data");  
    List<String> cols = Arrays.asList(tp.getSinkColumns().split(",")); 
    data.keySet().removeIf(k -> !cols.contains(k));  
    // 将需要的数据从bean中提取到json中
    jsonObject.put("sink_table", tp.getSinkTable());  
    jsonObject.put("sink_family", tp.getSinkFamily());  
    jsonObject.put("sink_row_key", tp.getSinkRowKey());  
    return jsonObject;  
}
```
### 4.3.6 向 HBase 写入维度数据
- 定义 HBaseSinkFunction
```java
// 涉及到连接，需要继承RichSinkFunction
// 在invoke方法中处理逻辑
public void invoke(JSONObject jsonObj, Context context) throws Exception {  
	// 当后续数据发生变化时，maxwell同步过来的增量数据
    String type = jsonObj.getString("type");  
    if ("delete".equals(type)) {  
        HBaseUtil.delCell(conn ,jsonObj);  
    } else {  
        HBaseUtil.putCell(conn, jsonObj);  
    } 
}

// 添加一行数据
public static void putCell(Connection conn, JSONObject jsonObject) throws IOException {  
  
    JSONObject data = jsonObject.getJSONObject("data");  
    String tableName = jsonObject.getString("sink_table");  
    String sinkRowKey = jsonObject.getString("sink_row_key"); // id
    String rowKeyVal = data.getString(sinkRowKey);  
    Table table = conn.getTable(TableName.valueOf(Constant.HBASE_NAMESPACE, tableName)); 
    Put put = new Put(Bytes.toBytes(rowKeyVal));  
    for (String col : data.keySet()) {  
        String v = data.getString(col);  
        if (v != null) {  
   put.addColumn(Bytes.toBytes(jsonObject.getString("sink_family")), Bytes.toBytes(col), Bytes.toBytes(v));  
        }  
    }  
    table.put(put);  
    table.close();  
}

// 删除一行数据
public static void delCell(Connection conn, JSONObject jsonObject) throws IOException {  
  
    JSONObject data = jsonObject.getJSONObject("data");  
    String tableName = jsonObject.getString("sink_table");  
    String sinkRowKey = data.getString("sink_row_key");  
    String rowKeyVal = data.getString(sinkRowKey);  
    Table table = conn.getTable(TableName.valueOf(Constant.HBASE_NAMESPACE, tableName)); 
    Delete delete = new Delete(Bytes.toBytes(rowKeyVal));  
    for (String col : data.keySet()) {  
delete.addColumn(Bytes.toBytes(jsonObject.getString("sink_family")), Bytes.toBytes(col));  
    }  
    table.delete(delete);  
    table.close();  
}
```
- 写出 `addSink(new HBaseSinkFunction())`


# 5 DWS
- FlinkSQL
	- 自定义函数 `UDTF`
	- 开窗聚合
- 上游是 `left join`，下游需要去重
	- 需求自带去重逻辑，不需要对 `left join` 产生的重复数据进行去重
		- 下单人数，支付人数（本身对 `uid` 进行了去重）
	- 需求只需要左表信息，任取一条即可，一般取第一条
	- 需求为累积型需求，输出过的数据后续输出 0
	- 上游 left join 时添加系统时间，下游去重时比较时间，将时间大的数据更新进状态，同时设置定时器用于最终发送数据（防止只有一条数据）
		- 有延迟，最后用
- 关联维表
	- 效率低，会产生反压，需要优化
	- 旁路缓存
		- 选型
			- Redis：复用、更新数据后的一致性
			- Flink 内存：`LRUCache`
		- 一致性
			- 先写入 `HBase`，再删除 `Redis`
				- 挂掉时会读到旧数据
			- 先删除 `Redis`，再写入 `HBase`
				- 挂掉时会读到旧数据
				- 有可能删除 `Redis` 后还未写入 `HBase` 时进行读取
					- 此时读不到，去 `HBase` 中读
					- 读到旧数据后又写入 `Redis`，造成数据不一致
			- 延迟双删
			- 先写 `Redis`，再写 `HBase`
	- 异步 IO
	- 数据量：为什么又使用旁路缓存，又使用异步 IO
		- 需求少，资源充足，压测时数据量多
# 总结
- 为什么做这个项目
	- 随着业务的发展，对数据实时性的要求越来越高，传统离线数仓的 $T+1$ 模式已经不能满足，所以需要实时数仓的能力
- 项目架构
	- 日志数据通过 `Flume` 读到 `Kafka`，然后通过 `Flink` 消费
	- 业务数据主要通过 `Maxwell` 同步到 `Kafka` 中，通过 `Flink` 消费
	- 也用到了 `HBase` 缓存维表，以及 `Doris` 存储 `DWS` 的结果
- 建模
	- 数据调研
		- 先从 Java 人员那里获取表，表中最好有描述字段或是说明文档，熟悉表中业务，梳理清楚业务线，找到事实表和维度表
		- 和业务相关人员验证所理解的业务是否正确
		- 和产品经理沟通，确定原子指标，派生指标，衍生指标
	- 明确数据域
		- 用户域：注册，登录
		- 流量域：启动，页面，动作，曝光，错误
		- 交易域：加购，下单，支付
		- 工具域：领用优惠券，使用优惠券
		- 互动域：评论，收藏
	- 构建业务矩阵：找事实表和维度表的关系
	- 建模
		- ODS
			- 将业务数据和日志数据保存在 Kafka 中
			- 保持数据原貌不做处理
		- DWD
			- 找指标中需要的业务过程
			- 声明粒度，保持最小粒度
			- 确定维度，找到指标中需要的维度
			- 确定事实表的度量值
			- 维度退化：通过 LookupJoin 将字典表的字段退化到明细表中
		- DIM
			- 将维度数据存储在 HBase，不维度整合
				- 拉链表：不需要历史切片数据
				- 维度整合：麻烦，优化读取
	- 指标体系建设
		- ADS
			- 考虑需求，日活，新增，GMV，留存转化等
		- DWS
			- 找派生指标，衍生指标
			- 选择业务过程，统计粒度相同的去 `Doris` 建宽表
	- 数据量
	- ODS 层
		- 1G -> 100W 条
		- 用户行为数据：73G
			- 曝光：44G
			- 页面：15G
			- 动作：7G
			- 故障 + 启动：7G
		- 业务数据：900M
			- 登录：18W，注册：900 左右
			- 加购，增量 18W，全量 90W，下单 9W，支付 7W，取消下单 500，退款 500
			- 领取优惠券 4W，使用优惠券下单 3W，使用优惠券支付 2.5W
			- 点赞评论收藏各 1000
			- 用户(活跃 85W，新增 900，总用户 900W)
			- 商品 SPU(1.6W)，商品 SKU(18W)
	- DWD 层 + DIM 层：和 `ODS` 层几乎一致
	- DWS 层：轻度聚合后，30G
	- ADS 层：十多 M，可以忽略不计
- 遇到的问题
	- 选型
		- ODS, DWD: Kafka
		- DIM
			- MySQL： 从库
			- Redis：内存
				- 用户表数据量大
			- Hive：慢
			- Doris：行查效率低
			- HBase：可行可列
				- 单个列族时是行存
				- 每个列族只有一列时为列存
		- DWS
			- 一个表会出多个不同指标
			- Kafka：每个指标会对应一个 `Flink` 应用程序，浪费资源
			- Doris：每个指标对应一个 `SQL` 语句