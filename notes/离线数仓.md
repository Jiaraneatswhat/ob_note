# 1. 使用到的小框架
## 1.1 DataX
### 1.1.1 原理
- `DataX` 是一个离线异构数据源同步工具
- 主要由三部分组成
	- Reader：数据采集模块，负责采集数据源的数据，将数据发送给 `Framework`
	- Framework：用于连接 `Reader` 和 `Writer`，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等问题
	- Writer：数据写入模块，负责不断从 `Framework` 取数据

![[datax1.svg]]

- 运行流程
	- Job：单个数据同步的作业，负责监控并等待多个 `TaskGroup` 模块任务完成，完成后成功退出
	- Task：`DataX` 作业的最小单元，`Job` 启动后根据不同的切分策略将 `Job` 分成多个 `Task`，每个 `Task` 负责一部分数据的同步工作
	- TaskGroup：切分完 `Task` 后，会通过 `Scheduler` 模块将 `Task` 组合成 `TaskGroup`，默认单个 `TaskGroup` 的并发数为 5
	- `TaskGroup` 启动 `Task` 后，通过 `Reader -> Channel -> Writer` 执行同步工作

![[datax2.svg]]

### 1.1.2 做过哪些优化
#### 1.1.2.1 全局
- 提升每个 `Channel` 的速度
	- `DataX` 内部限制
		- 每秒同步的 `record` 的个数
		- 每秒同步的字节数
- 可以将单个 `Channel` 的速度上限增加到 `5MB/s`
```Python
{
"speed":{
	"channel": 2,  # 此处为数据导入的并发度，建议根据服务器硬件进行调优
	"record": -1, # 此处解除对读取行数的限制
	"byte": 5242880, # 此处解除对字节的限制
	"batchSize": 204 # 每次读取batch的大小
	}
}
```
#### 1.1.2.2 局部
- 提升 `Job` 内 `Channel` 的并发数
	- 配置全局 `Byte` 限速以及单` Channel Byte` 限速
		- `Channel 数 = 全局 Byte 限速 / 单 Channel Byte 限速`
	- 配置全局 `Record` 限速以及单 ` Channel Byte` 限速
		- `Channel 数 = 全局 Record 限速 / 单 Channel Byte 限速`
	- 直接配置 `Channel` 个数
#### 1.1.2.3 内存
- 执行 `datax.py` 时直接加参数
	- `python datax/bin/datax.py --jvm="-Xms8G -Xmx8G" /path/to/your/job.json`
### 1.1.3 遇到过什么问题
- Null 值问题
	- MySQL 中的 `null` 值就是 `null`，而 `Hive` 中的 `null` 值用'\\N'来存储
	- MySQL -> Hive
		- 建表时手动指定 `null` 值
		- `null defined as ''`
	- Hive -> MySQL
		- 在 DataX 的 hdfsreader 中指定属性
		- `"parameter":{"nullFormat: ""}`
### 1.1.4 与 Sqoop 的比较
- Sqoop 的原理：
	- 仅 `Mapper` 任务的 `MR`
	- 自定义了 `InputFormat` 和 `OutputFormat`
- `DataX` 是一个多进程的线程
- `Sqoop` 支持分布式，`DataX` 可以通过调度系统实现
- `DataX` 有流控功能
- `DataX` 有统计信息和数据校验
### 1.1.5 DataX 为什么不使用增量同步
- DataX 的增量同步是新版本才有的功能，以前搭好的采集系统没有更改

## 1.2 DolphinScheduler
### 1.2.1 DS 挂了怎么办
- 查看日志报错原因
- 一般是资源不够导致，增加资源后重启
### 1.2.2 调度的任务挂了怎么办
- 配置告警及时处理
- 查看日志解决问题
- 重新跑任务
### 1.2.3 什么时候开始调度
- 需要调度的属于全量同步
- 业务数据：每天的 00: 00开始 ---->  `DataX` 全量同步
- 日志数据：每天的 00：30 开始 ----> `Load` 命令将数据加载到 `ODS` 层
	- 在 Flume 中的 HDFS Sink 中设置的滚动文件的周期是 30min
	- 需要等待 30min 防止没有滚动结束
# 2. 每天的指标有多少
- 100 个指标，节假日、活动时有大概 150-200 个
- 从用户来说
	- 统计用户的变动情况，回流，流失等
	- 统计用户留存率
	- 统计 1/7/30 日用户的新增和活跃情况
	- 用户的行为漏斗分析
- 从商品来说
	- 统计最近 30 日的复购率
	- 统计 1/7/30 日各品牌下单情况
	-  统计 1/7/30 日各品类下单情况
- 从交易相关来说
	- 统计下单到支付的平均时间间隔
	- 各省份的交易统计
- 从用户的行为日志中
	- 统计各渠道的流量
	- 路径分析
# 3. 建模准备
## 3.1 模型
- ER 模型
	- 三范式
		- 第一范式：属性不可分割，保证了数据的可用性
		- 第二范式：非主键字段完全依赖主键，不能存在部分函数依赖，减少了数据冗余
		- 第三范式：非主键字段不能传递依赖于主键，不能存在传递函数依赖，保证了数据的一致性
- 维度模型
- 星型模型：事实表只有一级维度

![[star_schema.svg]]

- 雪花模型：事实表存在多级维度

![[snowflake_schema.svg]]

- 星座模型: 多张事实表存在相同的维度表

- 事实表
	- 概念
		- 由用户行为产生的业务过程数据
	- 导入方式
		- `maxwell` 增量
		- 特殊：加购表为存量型指标，使用全量和增量
	- 种类
		- 事务型事实表
			- 选择业务过程 -> 声明粒度 -> 确定维度 -> 确定事实
			- 缺点：
				- 存量型指标
				- 多事实关联: 需要多张表 `join`
		- 周期型快照事实表
			- 购物车存量
		- 累积型快照事实表
			- 订单下单到完成时间
- 维度表
	- 概念
		- 描述事实发生时的环境信息
	- 导入方式 
		- 全量
		- 特殊：增量 用户信息
	- 维度整合
		- 星型模型需要维度退化
		- ER 模型拆开的表需要整合
		- 商品信息表：`SKU SPU Trademark Category1, 2, 3`
		- 省份信息表：大区表，省份表
		- 活动信息表：活动信息表 活动规则表
	- 拉链表：方便获取历史切片数据
		- 数据量大的缓慢变化维
- 全量同步和增量同步的选择
	- 数据量
		- 小：全量
		- 大：是否会发生修改操作
			- 是：变化的多少
				- 多：全量
				- 少：增量 + 拉链
			- 否：增量
# 4. 数仓建模
## 4.1 业务调研
- 所有的业务数据(最好有建表语句)
- 与 Java 部门沟通
- 与组长或项目经理沟通指标
## 4.2 明确数据域
- 用户域：注册，登录
- 流量域：启动，页面，曝光，动作，错误
- 交易域：加购，下单，支付
- 工具域：优惠券相关
- 互动域：评价，收藏
## 4.3 构建业务矩阵
- 找事实表和维度表的关系
## 4.4 维度建模(ODS -> DWD/DIM)
## 4.5 指标体系建设(ADS -> DWS)
- 原子指标：业务过程 + 聚合逻辑 + 度量
- 派生指标：原子指标 + 统计周期 + 统计粒度(维度组合) + 业务限定
- 衍生指标：派生指标再加工   eg：留存率
- 业务相同的一类指标可以放在同一张 `DWS` 表中


# 5. 架构

![[framework.svg]]

- 日志数据保存在磁盘文件中，`Flume` 通过 `TailDirSource` 实现断点续传将数据发送给 `Kafka`
- `Kafka` 直接从 `KafkaChannel` 中接收数据，可以省略一个 `Sink`，传输效率更高
- `Kafka` 的作用主要是用于解决读写速度不一致的问题，起到一个缓冲作用
- 为了解决 `Flume` 访问数过多，占用线程池的情况，再通过第二层 `Flume` 将日志数据存储到 `HDFS`
- 第二层 `Flume` 通过 `FileChannel` 将数据持久化到磁盘，通过一个拦截器来解决零点漂移的问题
## 5.1 框架选型
- Apache
- 云服务器：阿里云，华为云(考虑公司和阿里腾讯合作关系)
- 框架版本
	- Hadoop：3.1.3
	- Zookeeper：3.7.1
	- Flume：1.10.0
	- Kafka：3.0.0
	- Hive：3.1.3
	- HBase：2.4.0
	- Spark：3.3.0
	- Flink：1.13.0
- 服务器选型：物理机
## 5.2 集群规模
### 5.2.1 数据量
- 用户行为数据
	- 用户日活 85W，每人一天平均 100 条，共计 8500W 条
	- 每条日志 0.9K，共 `8500W * 0.9 = 73G`
	- ODS 采用 `gzip` 压缩，`73G` 压缩至 `22G` 左右
	- DWD 采用 `snappy + orc` 存储，`25G` 左右
	- DWS 层不足 `10G`，按 `10G` 算 
	- ADS 层忽略不计
	- 保存 3 副本：`57 * 3 = 171G`
	- 1 年内不扩容服务器：`171G * 365 天 = 约 61T`
	- 预留 20%-30%的缓存 `61 / 0.7 = 约 87T`
- Kafka 中的数据
	- 每天约 `73G 数据 * 副本数 2 = 146G`
	- 保存 3 天 * 146G = 438G
	- 预留 30% 的缓存 438 / 0.7 = 623G
- 业务数据
	- 每天活跃用户数 85W，下单 10W，每人产生业务数据 10 条，每条 0.9 k 左右，`10W * 10 * 0.9 = 1G 左右`
	- 数仓五层存储：1G * 3
	- 保存 3 副本：3G * 3
	- 1 年内不扩容服务器：9G * 360 = 约 3.2T
	- 预留 20-30%的缓存：5T
- 总数据量：96T 左右