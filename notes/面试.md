### 5.1 Linux 高级命令
- 查看内存
	- `top`
	- `free -m`
		- 默认使用 `kb` 显示大小
		- `-m` 指定使用 `mb` 显示大小
	- `jmap -heap` 查看 `java` 进程的堆内存
- 查看磁盘
	- `df -h` ：`-h`: `human-readable`
	- `du -sh`：`-s`: `summary`
- 查看端口
	- `netstat -anp`
		- `-a`: all
		- `-n`: 不显示别名
		- `-p`: 显示 `pid, pname`
- 查看进程
	- `ps -ef`
		- `-e`: 显示全部
		- `-f`: 显示全部字段
- awk
	- `awk '{pattern + action}' <file>`
	- 默认分隔符是 `' '` 和 `tab`
	- 用 -F 可以指定分割符
		- `awk -F ':|,' 'action' <file>` 用 `:` 或 `,` 分割
	- 内置变量
		- `FILENAME`
		- `$0`: 当前整行
		- `NR`: 当前已读的行数
		- `NF`: 当前行被分割的列数
		- `BEGIN, END`: 在 `awk` 程序开始前和结束后执行一次
			- 可以计算统计值
			- `awk 'BEGIN {max=0} {if($2>max) max=$2} END {print "max: ", max}' <file>`
- sed
	- 流编辑器，一次处理一行的内容
	- 主要用于自动编辑一个或多个文件，简化操作
	- `sed [options] '[地址定界] command' file`
	- options
		- `-n`: 只打印匹配到的行
		- `-e`: 多次匹配
		- `-i`: 直接将处理结果写入文件
	- 地址定界
		- 默认全文
		- /pattern/
			- `sed -n '/hello/p' hello.txt`
		- 范围:  #,#
	- command:
		- `d`: 删除匹配到的行
		- `p`: 打印
		- `s/old/new/g`：将 old 替换 new, g 表示全局
			- `sed 's/hello/HELLO/g' hello.txt` 
- `' '` 和 `" "`
	- 单引号不解析参数，双引号会
	- 嵌套使用: `"' $1'", '"$1"'` 按照外层
### 5.2 读写流程
- 写流程
	- `Client` 创建一个 `DataStreamer` 用于后续写数据
	- `Client` 通过 `RPCServer` 向 `NN` 发起写请求
	- `NN` 检查客户端的权限，确认目录树中不存在要写的文件时，创建一个 `INode` 节点
	- `Client` 开始写，生成 `packet` 后，`DataStreamer` 会建立管道向 `NN` 请求块地址
	- `NN` 根据机架感知将块所在的 `DN` 地址返回给 `Client`
		- 优先同机架
		- 其次另一个机架
		- 最后另一个机架的另一个节点
	- `Client` 和 `DN` 建立连接，获取到契约后开始写数据，`DN` 通过 `DataXceiver` 接收数据，返回 `ack`，将数据持久化到磁盘
- 读流程
	- 客户端创建输入流，向 `NN` 建立连接获取块地址
	- `NN` 检查权限和目录树后，返回块地址
	- 客户端和 `DN` 建立连接读取数据
### 5.3 小文件问题
- 危害
	- 存储：每个块对应 150byte 的元数据信息，会占用 NN 内存
	- 计算：每个小文件切一片
- 解决
	- 源头：
		- 通过 Flume 向 HDFS 写时
			- 设置三个滚动参数来避免小文件
		- ADS 层的 SQL：今日数据 `union all` 昨天之前的数据进行 `overwrite`
		- 手动合并
		- `har` 归档
			- `hadoop archive -archiveName name <src> <dest>`
	- 计算
		- JVM 重用(只适用于 `MR` 进程)
		- `CombineTextInputFormat`
### 5.4 Shuffle 优化
- 流程
	- `map` 方法中通过 `context` 写出后，数据通过分区器收集到环形缓冲区中
		- 环形缓冲区是一个数组，存放着 `kv` 的序列化数据和 `kv` 的元数据信息，由 `equator` 进行分割
		- `kv` 数据按索引增大的方向存储，`kvmeta` 按索引递减的方向存储
	- 在环形缓冲区中进行写时会进行快排，实现分区内有序
	- 达到 80% 的阈值后，进行归并排序生成单个大文件，向磁盘溢写
	- `Reduce` 阶段分为三部分：`copy, merge, reduce`
	- `copy` 阶段通过 `LocalFetcher` 线程拉取数据
	- `merge` 阶段开启两个 `Merge` 线程，一个向内存溢写，一个向磁盘溢写，在溢写时进行归并排序
	- 最后进入 `reduce` 阶段输出结果
- 优化
	- 环形缓冲区默认大小 100M，可以增大到 200M
		- 切片在最后一片时，如果剩余的数据量 < 块大小的 1.1 倍，会将剩余部分与上一片合在一起，可以减少溢写的次数，防止归并排序
		- 若溢写的次数仍然较多，说明文件很大且不能切分
	- `merge` 的默认合并次数是 10，可以提高到 20
	- 不影响业务逻辑的前提下，可以在第一次归并后使用 `Combiner`
	- 当 `IO` 遇到瓶颈时，可以进行压缩
	- 增加 `Container` 的内存和 `CPU` 资源
	- `reduce` 阶段可以提高去 `Map` 拉取数据的并行度，5 -> 10
	- 可以提高 `copy` 阶段 `Buffer` 占 `Reducer` 内存的比例，0.7 -> 0.8
### 5.5 Yarn 工作机制
- `Yarn` 基于服务化框架和事件驱动模型
	- 设计定义了一个抽象服务类，用户只需要实现其中的 `serviceInit()`, `serviceStart()` 等方法即可
	- 设计了一个接口事件类，所有事件都通过实现该接口来表示状态，产生的事件放在队列中，由消费者通过 `Handler` 进行处理
	- 一个对象对应的所有的状态变化汇总起来就是状态机，`Handler` 会使用状态机来更新对象状态
- 任务的执行流程
	- `Client` 向 `Yarn` 申请提交 `Job`
	- 进行切片，上传配置文件
	- `YarnRunner` 封装启动 `AM` 的命令，申请开启一个 `Application`
	- 状态机处理事件，`RMApp -> RMAppAttempt -> Container`
	- 从 `RM` 申请到资源后，选择 `NodeManager` 启动 `AM`
	- `AM` 启动后执行 `Job`，产生事件交给状态机处理，仍然是申请资源，选择 `NM` 开启 `YarnChild` 进程执行任务
### 5.6 调度器特点
- FIFO：单队列，不用
- Capacity：多队列，队列内部 `FIFO`，用于并行度不高的场景
- Fair：多队列，按照缺额分配，并行度高
- 在 `yarn-site.xml` 中配置队列
- 队列的设置
	- 框架，业务线，部门，人员

4 选举机制
	- `QuorumPeer` 进程启动后，首先从磁盘恢复数据到内存，初始化的 `ServerState` 是 `LOOKING`
	- 准备选举
		- 产生一个 Vote 对象，传入自己的 `myid`
		- 选择选举算法，配置类 `QuorumPeerConfig` 中设置为 `FastLeaderElection`
		- `FastLeaderElection` 实例化时
			- 创建两个阻塞队列 `sendQueue` 和 `recvQueue` 用于存放选票
			- 创建两个线程 `WorkerSender` 和 `WorkerReceiver`
		- `FastLeaderElection` 的 `start()` 会启动上述两个线程
		- `WorkerSender` 从 `sendQueue` 中取出 `ToSend` 对象，决定发送给别人还是放在自己的 `recvQueue` 中
		- `WorkerReceiver` 保存选票到 `recvQueue` 中
	- 开始选举
		- 初始化选票时，投自己一票
		- 广播选票，创建一个 `ToSend` 对象放入 `sendQueue` 中
		- 交换选票，从 `recvQueue` 中取出选票，先比较 `epoch`, 再比较 `zxid`, 最后比较 `myid`，更新选票并广播
		- 选票超过半数后，将 `peerId` 与选票推选的 `Leader` 相同的节点设置为 `Leader`，其他的为 `Follower` 
	- 选举收尾
		- `Peer` 成为 `follower` 后，创建 `Follower` 对象，与 `leader` 建立连接
		- `Peer` 成为 `leader` 后，创建 `Leader` 对象，与 `follower` 保持心跳
## 6.1 基本信息
### 6.1.1 生产流程
- 两个线程：`main`， `sender`
- 拦截器
- 序列化器
- 分区器
	- 默认 `DefaultPartitioner`
		- `ProducerRecord` 中指定分区直接发往指定分区
		- 未指定分区，但是有 Key，通过 hash 值
	- 粘性
	- 轮询：每次加 1 后取余，避免数据每次第一次发送给同一个分区
- ack
	- -1(all): `Leader` 和 `ISR` 队列中全部的副本收到数据，进行响应
	- 0：生产者发送过来的数据不需要等待数据落盘应答
	- 1：生产者发送过来的数据，`Leader` 收到数据后应答
### 6.1.2 Broker 相关
- 主题：逻辑上的分类
- 主题名-分区号作为目录名进行物理存储
- 分区：提高吞吐量，增加了负载均衡的能力
- 副本：提高可靠性
	- `ISR + OSR = AR`
	- `LEO`：每个副本最大的 `offset + 1`
	- `HW`: 同一个分区中，多个副本最小的 `LEO`
		- 消费者可见的最大 `offset`
		- 当需要消费的 `offset` 超过记录的 `offset` 时，会触发  `auto.offset.reset`
		- 第一次启动时(新的消费者组), 没有初始的 `offset` 时也会触发
		- 保证了存储数据的一致性

### 6.1.3 消费者
- 分区分配策略
	- `RangeAssignor`：按范围分
	- `RoundRobinAssignor`：对消费者进行排序后，轮询消费
	- `StickyAssignor`：在重新分区时，尽量保证原有分区与消费者之间的绑定关系不变
- Offset 存储
	- 老版本：`zk`
	- 新版本：`__consumer_offsets` 50 个分区
	- 手动维护：`Flink：Checkpoint`
### 6.1.4 生产环境相关
- 数据量(高峰期以 100W 为例)
	- 100W 日活，每个用户 100 条数据共计 1 亿条数据
	- 1 条 / k  共计 100G 左右
	- 离线最大的表是行为日志
	- 实时关心的是高峰期数据
	- 平均 `1亿 / (3600 * 24) = 1150 条/s`
	- 高峰期速度是均值的 2-20 倍  2000-20000 条  20M/s
- 主题数
	- ods: 2
	- dwd: 
		- 日志分流: 启动、页面、曝光、错误、行为产生 5 个
		- 业务主题分流产生 4 个
		- 大概 20-30 个
- 副本数 2-3
- 分区数按主题分
	- 日志数据主题数据量最大，6 个分区
	- 其他主题，2-3 个分区
- 压测
	- `Kafka` 自带压测脚本
	- `kafka-producer-perf-test.sh` (<font color='red'>perf</font>ormance)
		- --record-size: 一条消息的大小(byte)
		- --num-records
		- --throughput: 每秒的吞吐量
	- `kafka-consumer-perf-test.sh` 
		- --broker-list
		- --topic
		- --fetch-size
		- --messages
- 机器台数: `峰值速度 / min(生产速率，消费速率)`
- 监控：通过 eagle
	- 生产、消费速度
	- lag: 消费数据与 `Kafka` 中 `offset` 的差值，是否存在数据积压问题
- 保存天数更改为 3 天
## 6.2 挂了
- 多副本可以保证数据不丢
- Flume，日志文件解耦
- 大部分情况下是内存的原因
## 6.3 数据丢失
- 生产者丢数据
	- `ack` 不是 -1
	- 解决：将 `ack` 设置为 -1，如果此时 `follower` 副本超过了同步时间，进入 `osr` 队列时，此时只会有 `leader` 进行响应，相当于 `ack` 变为了 1
	- 因此还需要 `isr` 中副本数 >= 2
- 消费者丢数据
	- 消费者消费到数据后，先保存 `offset`，再保存数据
	- 保存 `offset` 后，任务挂掉，重启后会接着 `offset` 进行消费，导致数据丢失
	- 解决：先保存数据，再保存 `offset`
## 6.4 数据重复
- 生产者
	- `ack` 设置为-1 时有可能导致数据重复
	- 解决：幂等，事务
- 消费者
	- 先保存数据，再保存 offset 有可能导致数据重复
	- 解决：事务写出，下游去重(幂等性)
	- `doris` `replace`
## 6.5 数据积压
- 生产速度 > 消费速度
- 生产环境只能提高消费速度
	- 提高单次拉取 `ConsumerRecord` 的个数 `max.poll.records, 默认500`
	- 提高单次拉取的数据量大小 `fetch.max.bytes, 默认50 * 1024 * 1024`
	- 数据的处理速度跟不上(数据倾斜，反压，关联维表)，不会去 fetch 下一次数据
	- 提高下游处理能力：增大资源，旁路缓存，异步 IO
## 6.6 其他
- Kafka 使用了两个系统调用
	- 直接内存映射 MMAP
	- sendFile 零拷贝
- 内存：10-15G
- 高效的原因：
	- 分区
	- 稀疏索引
	- 页缓存
	- 零拷贝
		- 传统的网络 I/O:
			- 操作系统从磁盘把数据读到内核区
			- 用户进程把数据从内核区拷贝到用户区
			- 用户进程将数据写入 `Socket`, 数据到内核区的 `Socket Buffer` 上
			- 最后将数据从 `Socket Buffer` 发送到网卡
		- `sendFile` 优化：
			- 直接把数据从内核区拷贝到 `Socket`
			- 然后发送到网卡
			- 避免了在内核 `Buffer` 和用户 `Buffer` 来回拷贝的弊端
	- 顺序写磁盘
- 能不能消费到 7 天前的数据
	- `Kafka` `segment` 的默认大小是 `1GB`
	- 到达默认的删除时间是，会按照文件进行删除，需要文件中的最后一条数据到达 7 天才会删除
	- 有可能一个文件中同时存在 7 天和 7 天前的数据
- 如果新增了一条维度信息，马上进行了修改，修改数据比新增数据先写出到 `HBase`，如何解决
	- 考察如何保证 `Kafka`  的数据有序性 
	- 将新增和更新的数据发往同一个分区 -> 保证 `key` 相同 -> 按主键作为 `Kafka` 的 `key` 值
	- `maxwell` 的配置文件中，通过 `producer_partition_by=primary_key` 设置
	- 仅限于 `HBase`：将事件时间作为版本写入到 `HBase` 中
- 来一条 2M 的数据，`Kafka` 会产生什么现象
	- 卡死，默认的最大的数据大小为 1M
## 6.7 生产流程
- 创建一个 `KafkaProducer` 对象，给 `Producer` 配置拦截器，序列化器，分区器，创建一个 `RecordAccumulator` 对象以及一个线程池，开启 `Sender` 线程，创建 `NetworkClient` 用于连接 `Broker`
- 调用 `send` 方法，数据经过拦截器，序列化器，分区器后发送到 `RecordAccumulator` 中，未达到数据发送条件时，`sender` 线程会阻塞
- 当数据达到 `batchSize` (`batch.size 默认16k`) 或到达等待时间(`linger.ms 默认0s`)，会唤醒 `Sender` 线程，通过 `Selector` 选择 `Channel` 向 `Leader` 发送 `ProducerRequest`
- `Leader` 响应 `Producer` 的请求，完成写入，返回 `ack`
## 6.8 Broker 工作流程
- `KafkaServer` 在启动时会创建 `Controller`，`ReplicaManager`，`LogManager`，`GroupCoordinator` 等组件并启动
- `Controller` 启动后会去 `Zk` 创建节点并注册 `Watcher`，先创建节点的成为 `leader`，如果 `leader` 节点挂了，`zk` 监听到节点的变化，就会触发重选举
- `Leader` `Controller` 所在节点的 `ReplicaManager` 会选举 `Partition` 的 `leader`，以 `isr` 中存活为前提，按照 `AR` 排在前面的优先
- `ReplicaManager` 会获取 `isr`，由 `controller` 将节点信息上传到 `zk`
## 6.9 消费者组消费流程
- `Consumer` 向 `GroupCoordinator` 发起 `joinGroup` 请求，先发送的成为 `leader` `consumer`
- `GroupCoordinator` 向 `leader consumer` 发送待消费主题的信息
- `leader consumer` 制定消费方案发给 `GroupCoordinator`
- `GroupCoordinator` 将消费方案发送给每个 `Consumer`
- 每个 `Consumer` 和 `GroupCoordinator` 保持心跳，一旦超时，或者处理消息时间过长，就会被移除并触发再平衡
#### 1.6 优化
##### 1.6.1 建表
- 分区：根据分区存储到不同的路径下，防止后续全表扫描
- 分桶：根据分桶的字段将数据存储到相应的文件中，对未知的复杂的数据进行提前采样
- 文件格式：列存(orc, parquet)
- 压缩
##### 1.6.2 写 SQL
- 单表：
	- 行列过滤(提前进行 where，不使用 select * )
	- 矢量计算：批量读取数据，默认 1024 条
	- map-side 聚合 `hive.map.aggr=true; 默认true`，预聚合，减少 `shuffle` 数据量
	- 某些没有依赖关系的 Stage 可以同时执行 `set hive.exec.parallel=true`
- 多表：
	- CBO (Cost Based Optimizer): 在 join 时计算成本，选择最佳的 join 方式，默认开启
	- 谓词下推
		- 尽量将过滤作前移，以减少后续计算的数据量
```sql
-- 过滤条件为关联条件，下推至两张表
select * from t1 join t2 on t1.id = t2.id where t1.id > 50
-- 过滤条件不是关联条件，只会下推到相关表
select * from t1 join t2 on t1.id = t2.id where t1.age > 50
-- left join 相同，无论什么join，只要是关联条件就会下推到两张表
select * from t1 left join t2 on t1.id = t2.id where t1.id > 50
select * from t1 left join t2 on t1.id = t2.id where t1.age > 50
-- 下推到两张表
select * from t1 left join t2 on t1.id = t2.id where t2.id > 50
-- 特殊情况，下推到t2表会导致结果发生变化
-- 2.x 谓词下推失效
-- 3.x 下推到相关表，将left join 变为 join
select * from t1 left join t2 on t1.id = t2.id where t2.age > 50
```
- 大表 join 小表(数据量 < 25M) -> map join
- 大表 join 大表 
	- SMB(Sort Merge Bucket) Map Join
	- 参与 join 的表是分桶表，分桶字段为 join 的关联字段
	- 分桶数有倍数关系，将相对小表分桶后尽量达到可以 merge 的条件，让每个桶小于 25M
- 整体：
	- 本地模式
	- Fetch 抓取(默认开启)，简单的 SQL 就不会执行 MR
	- 严格模式
	- 调整 Mapper 个数：
		- `splitSize = max(1, min(blockSize, LONG_MAX_VALUE))`
		- 增加 `Mapper` 个数需要减小 `splitSize`，`blockSize` 一般不做更改，减小 `LONG_MAX_VALUE`
		- 减少 `Mapper` 个数需要增大 `splitSize`，增大 1
	- 调整 Reducer 个数：
		- 默认 -1
		- $min(ceil(\frac{totalInputBytes}{bytesPerReducer}),maxReducers)$
		- `hive.exec.reducers.bytes.per.reducer` 默认 256M
		- `hive.exec.reducers.max` 默认 1009
##### 1.6.3 数据倾斜
- 现象：绝大部分 Task 已经完成，只有一个或少数几个没有完成，且执行很慢，甚至有 OOM
- 原因
	- 单表 `group by`
	- 多表 `join`
	 ![[unoptimized_big2big_join.svg]]
	 
	- map 端文件不可切时，文件大小差距也可能造成数据倾斜
		- 解决方法：HDFS Sink 限制了单个文件的大小
- 解决
	- 单表
		- 不影响业务逻辑，可以进行一次预聚合
		- 否则，可以给分区字段添加随机数实现双重聚合，先对随机数拼接字段进行一次分组聚合，打散数据，再进行第二次聚合
	- 多表
		- 大表 join 小表 -> map join
		- 大表 join 大表
			- 不能使用 SMB Map Join，因为分桶后 join 时仍然是倾斜的
			- 相对大表加随机数打散，相对小表加随机数扩容
	![[optimized_big2big_join.svg]]
	
- 去重原理
	- 指定 distinct 时，Hive 会首先将数据从文件中读取到内存缓冲区
	- Hive 按照指定的列或表达式对数据进行分组
	- 组内按 Hash 算法或排序算法对数据进行排序或分桶，以便快速锁定重复行
	- 选择其中一个重复的行输出

## 9.1 架构
- 外部：`ZK`,  `HDFS`
- 内部：
	- `HMaster`：管理元数据
	- `RegionServer`：管理数据和 Region
		- `WALFactory`：处理和 WAL 相关的请求
		- `BlockCache`
		- `Region`： 表的切片
			- `WAL`
			- `Store`：列族的切片
				- `MemStore`
				- `StoreFile`
## 9.2 写流程
- 客户端首先去内存中寻找元数据信息，刚启动时找不到(4.2.4)
- 与 `zk` 通信，获取 `meta` 表所在的 `RegionServer` (4.2.6)
- 向 `meta` 表所在的 `RS` 发起写请求，获取 `meta` 表的内容(要写的表在哪个 `RS`) (4.3.1)
- 将 `meta` 表内容缓存在本地(4.3.2)
- 通过 `RPC` 向 `Server` 发送 `put` 请求
- 首先写 `WAL`，将数据写入本地缓存(4.4.2.3)，将缓存写入 `HDFS` (4.4.3)，再将数据同步到磁盘(4.4.4)
- 最后将数据写入到 `MemStore` (4.4.5)
## 9.3 读流程
- 缓存 `meta` 表为止的步骤与写流程相同
- 向待写入表发起 Get 请求
- `Server` 收到 `Get` 请求，创建 `MemStore` 和 `StoreFile` 的 `Scanner`
	- `MemStore` 的 `Scanner` 在内存中直接读取
	- `StoreFile` 的 `Scanner` 
		- 首先通过布隆过滤器读取文件的索引部分，对要检索的行的信息进行索引，判断文件中是否有要查询的行，接着根据信息找到行所在的数据块
		- 根据 `Block` 的 `id` 判断是否已经在 `BlockCache` 中缓存过，缓存过的情况下不会读取 `StoreFile`，否则从 `StoreFile` 中扫描 `Block` 进行缓存
	- 因为 `HBase` 中的数据存在版本，查到的数据不一定是版本最大的，因此将从 `MemStore`，`StoreFile`，`BlockCache` 中查到的所有数据进行合并
		- 所有指数据是同一条数据的不同版本 (`ts`) 或不同的类型 (`PUT` / `DELETE`)
	- 返回合并结果(非 `DELETE` 数据)
## 9.4 刷写
- RegionServer 级别：到达堆内存的 `40% * 95% = 38%` 时刷写
	- 达到堆内存的 40% 时会阻塞写入，直到降低到 38% 以下
- Region 级别：
	- 某个 `MemStore` 到达 128M 时，所在 `Region` 的所有 `MemStore` 都会进行刷写
- HLog 文件数达到 32
- 官方不建议使用过多 `ColumnFamily`，是因为当一个 `MemStore` 达到 128M，而其他 `MemStore` 的大小还很小时，刷写就会产生大量的小文件
## 9.5 合并
- 合并是从一个 Region 的一个 Store 中选取部分 HFile 文件进行合并
- 合并有两种：Minor Compaction 和 Major Compaction
	- `MemStore` flush 操作结束后会检查当前 `Store` 中 `StoreFile` 的个数，一旦超过了 ` hbase.hstore.compactionThreshold 默认3 `，就会触发合并
	- RS 会在后台启动一个 `CompactionChecker` 线程定期触发检查对应的 `Store` 是否需要执行合并，对应的参数
		- `hbase.server.thread.wakefrequency` 默认 10000ms
		- `hbase.server.compactchecker.interval.multiplier` 默认 1000
		- 不满足 `compactionThreshold` 的条件时，就会去检查是否需要 Major 合并
- Major 合并的参数
	- `hbase.hregion.majorcompaction` * `hbase.hregion.majorcompaction.jitter 默认.50F`
	- 默认 7 天进行 Major 合并
- Minor Compaction 是指选取部分小的、相邻的 `HFile`，将它们合并成一个更大的 HFile
- Major Compaction 是指将一个 `Store` 中所有的 `HFile` 合并成一个 `HFile`，这个过程还会完全清理三类无意义数据：被删除的数据、`TTL` 过期数据、版本号超过设定版本号的数据
## 9.6 切分
- 每个 Table 初始只有一个 Region，随着数据的不断写入，Region 会进行自动切分
- 切分策略的类都继承自 `RegionSplitPolicy`，有两个方法
	- `shouldSplit()`
	- `getSplitPoint()`：返回 `row` 的值，将所有 `row` 分为两段
- 切分的时机： `hbase.regionserver.region.split.policy`
	- 默认 `org.apache.hadoop.hbase.regionserver.SteppingSplitPolicy`
	- 如果 RS 只有一个 Region，按照 `2 * hbase.hregion.memstore.flush.size 刷写memstore大小` 进行切分
	- 否则按照 `habse.hregion.max.filesize 默认10G` 进行切分
## 9.7 数据删除时机
- eg:
```shell
# 向 HBase 中 put 一条数据
put 'table', 'rk', 'colFamily:xxx', 'val1' 
# 更改数据
put 'table', 'rk', 'colFamily:xxx', 'val2'
# 通过 scan 查看历史版本数据
scan 'table', {RAW=>TRUE, VERSIONS>=2}
# 返回
ROW           COLUMN+CELL
rk            column=colFamily:xxx, timestamp=ts2, value=val2
rk            column=colFamily:xxx, timestamp=ts1, value=val1 
# 手动flush, 再进行查询时只能查到val2
flush 'table'
# 再更改一次数据
put 'table', 'rk', 'colFamily:xxx', 'val3'
# flush后查看
ROW           COLUMN+CELL
rk            column=colFamily:xxx, timestamp=ts3, value=val3
rk            column=colFamily:xxx, timestamp=ts2, value=val2 
```
- 原因
	- 第一次刷写后的 `val2` 在文件中，第二次再进行刷写时无法删除
	- 刷写会删除在同一个 `MemStore` 中的过期数据
	- 而合并会删除所有过期数据，`Major` 合并会删除数据的删除标记
## 9.8 RowKey 设计
### 9.8.1 设计原则
- 唯一性，`rowkey` 需要包含事实的主键列
- 散列性
- 长度
- 范围查询的需求应尽量连续紧凑分布
- 随机查询的需求应尽量散列分布，保证负载均衡
### 9.8.2 场景题
#### 9.8.2.1 手机号查询指定事件通话记录
- 预分区(散列性)
	- 00|, 01|, 02|... ‘|’的 `ascii` 码为 124
	- 评估数据量，保证未来单个 `Region` 不超过 10G，同时考虑机器台数，尽量让 `Region` 数为机器台数的整数倍，保证均匀分布
- 分区号
	- 00_, 01_, 02_,...     '\_'的 `ascii` 码为 95
	- 轮询可以保证散列，但是查询困难
	- 考虑查询条件，手机号年(月日)
		- (手机号 + 年月日).hash() mod 分区数  
			- 按年份查找时需要跨分区数过多
		- (手机号 + 年月).hash() mod 分区数
			- 按天和按月查只需要一个分区
			- 按年最多也只需要扫描 12 个分区
		- (手机号 + 年).hash() mod 分区数
		- 手机号.hash() mod 分区数
- 拼接字段
	- 考虑查询条件
		- 0X_13930391234_2023-12-12 12:00:00
		- 0X_2023-12-12 12:00:00_13930391234
	- 手机号在前：
		- start_key: 0X_13930391234_2023-12
		- stop_key: 0X_13930391234_2023-12|
		- 不会多数据，也不少数据
	- 日期在前
		- start_key: 0X_2023-12-00 00:00:00_13930391234
		- stop_key: 0X_2023-13-00 00:00:00_13930391234
		- 有可能混入其他手机号
#### 9.8.2.2 消费金额
```sql
+-----+-------+---------------------+
| id  |  user |         date        | 
+-----+-------+---------------------+
|  1  |   a   | 2022-01-05 09:00:00 |
|  2  |   b   | 2021-12-30 08:00:00 |
|  3  |   c   | 2022-01-04 09:08:00 |
|  4  |   d   | 2021-12-31 09:08:00 |
+-----+-------+---------------------+
```
- 统计某个用户在某个月份消费的总金额
	- id 放在开头，订单不一定连续，不满足紧凑分布的条件
	- start_key: 0X_a_2022-01
	- stop_key: 0X_a_2022-01|
- 统计所有用户在某个月份消费的总金额
	- start_key: 0X_2022-01
	- stop_key: 0X_2022-02
### 9.8.3 项目中 RowKey 的设计
- 需求：主流根据维度表的主键查询单条明细数据
	- 不需要考虑集中性
	- 对于数据量大的表可以考虑预分区
	- 使用 `MySQL` 维表的主键作为 `RowKey`
	- 如果是预分区表，需要将主键 `Hash` 取分区号拼接原先的主键作为 `RowKey
# 9. 优化
## 9.1 资源调优
### 9.1.1 内存设置
- JobManager 内存模型
- 进程内存= `Flink` 内存 + `JVM` 内存
- `Flink` 内存 = 框架堆内堆外内存 + `Task` 堆内堆外内存 +  网络缓存内存 + 管理内存
- `JVM Overhead = 总 mem * 0.1`
- `Network = Flink内存 * 0.1`
- `Managed Memory` 主要用于 `RocksDBStateBackend` 使用，可以适当调至 `0`
### 9.1.2 合理利用 CPU 资源
- 容量调度器默认使用 `DefaultResourceCalculator`，只根据内存来调度资源，因此资源管理页面上每个容器的 `vcore` 数为 1
- 每个 `TM` 中的 `Slot` 共用一个 `CPU`
- 可修改为 `DominantResourceCalculator`，每个 `Slot` 分一个 `CPU`
```xml
<!-- capacity-scheduler.xml -->
<property>
 <name>yarn.scheduler.capacity.resource-calculator</name>
 <value>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</value>
</property>
```
- 使用 `DominantResourceCalculator` 在提交任务时可以手动指定 
	- `-Dyarn.containers.vcores=1`
### 9.1.3 并行度设置
- 全局并行度
	- 总 `QPS` (每秒查询率) / 单并行度处理能力 = 并行度
	- 根据高峰时期 `QOS`，并行度 * 1.2 倍 
- Source 对接 `Kafka`，并行度设置为 `Kafka` 对应 `Topic` 的分区数
- Transform
	- `KeyBy` 前一般和 `Source` 保持一致
	- `KeyBy` 后，并行度尽量设置为 2 的整数幂
- Sink
	- 写 `Kafka`：与分区数相同
	- 写 `Doris`：批处理，3s 开窗聚合，数据量小，并行度足够
	- 写 `HBase`：缓慢变化维(`SCD`)
### 9.1.4 最终的资源配置
- JobManager: `1 CPU` `2-4G`
- TaskManager：
	- 根据 `Kafka` 分区决定并行度(一个 `Slot` 共享组)，`Slot 数 = 并行度`
	- 资源充足或数据量大 `cpu : slot = 1 : 1`
	- 反之 `cpu : slot = 1 : 2`
	- `1 CPU` `4G` 
## 9.2 状态及 Checkpoint 调优
### 9.2.1 RocksDB 大状态调优
- `RocksDB` 是内存 + 内盘，可以存储大状态
#### 9.2.1.1 开启 State 访问性能监控
- 可以查看有状态算子的读写时间
- 提交时使用参数 `-Dstate.backend.latency-track.keyed-state-enabled=true`
#### 9.2.1.2 开启增量检查点和本地恢复
- `RocksDB` 是目前唯一可用于支持有状态流处理应用程序增量检查点的状态后端
- 开启参数
	- `-Dstate.backend.incremental=true`
	- `-Dstate.backend.local-recovery=true` 不去 `HDFS` 拉取数据，基于本地状态信息恢复任务
	- `-Dstate.backend.latency-track.keyed-state-enabled=true`
- 预定义选项：
	- `DEFAULT`
	- `SPINNING_DISK_OPTIMIZED`
	- `SPINNING_DISK_OPTIMIZED_HIGH_MEM`
	- `FLASH_SSD_OPTIMIZED`
	- 预定义选项达不到预期后，可以调整下面的参数
- 调整预定义选项
	- 增大 `Block` 缓存
		- `state.backend.rocksdb.block.cache-size 默认8m` 设置到 `64-256MB`
	- 增大 `writeBuffer` 和 `level` 阈值大小
		- 每个 `State` 使用一个列族，每个列族 `writeBuffer` 的大小为 `64MB`
			- `state.backend.rocksdb.writebuffer.size`
		- 调整 `Buffer` 需要适当增加 `L1` 的大小阈值，默认 `256MB`
			- 太小会造成存放的 `SST` 文件过少，层级变多
			- 太大会造成文件过多，合并困难
			- `state.backend.rocksdb.compaction.level.max-size-level-base`
	- 增大 `writeBuffer` 数量，L0 中表的个数，默认 2，可以调大到 5
		- `state.backend.rocksdb.writebuffer.count`
	- 增大后台线程数，默认 1，可以增大到 4
		- `state.backend.rocksdb.thread.num`
	- 增大 `writeBuffer` 合并数，默认 1，可以调成 3
		- `state.backend.rocksdb.writebuffer.number-to-merge`
	- 开启分区索引
		- `state.backend.rocksdb.memory.partitioned-index-filters: true`
### 9.2.2 Checkpoint 设置
- 间隔时间可以设置为分钟级别(`1 - 5min`)
- `Sink` 使用事务写出时，`ck` 的时间可设置为秒级或毫秒级
- 超时时间
- Barrier 的对齐(减少状态大小) + 超时时间，超时后转为非对齐
	- `setCheckpointTimeout()`
	- `enableUnalignedCheckpoints()`
- 重启策略
	- 默认间隔 1s 重试 `INTEGER.MAX_VALUE` 次
	- `setRestartStrategy()` 每隔 5s 重试 3 次
## 9.3 反压
### 9.3.1 现象
- `WebUI` -> `BackPressure`
	- 0 - 10% 绿
	- 10 - 50% 黄
	- 50% 以上红
	- 多个 SubTask 可能全红，也有可能有红有绿

![[bkpressure.png]]
- `WebUI` -> `Metrics`
	- `outPoolUsage`：发送端 `Buffer` 的使用率
	- `inPoolUsage`：接收端 `Buffer` 的使用率
	- `floatingBuffersUsage` (1.9+)：接收端 `Floating Buffer` 的使用率
	- `exclusiveBuffersUsage` (1.9+)：接收端 `Exclusive Buffer` 的使用率
	- `inPoolUsage = floatingBuffersUsage + exclusiveBuffersUsage`
	- 如果 `SubTask` 发送端 `Buffer` 占用过高，说明受到下游反压限速
	- 如果 `SubTask` 接收端 `Buffer` 占用过高，说明传导反压至上游
- 定位问题
	- 禁用任务链
	- 一般是反压的最后一个 `Task`
	- 可以设置参数开启火焰图
		- `-Drest.flamegraph.enabled=true`
	- 也可以设置 JVM 参数，打印 GC 日志进行分析
		- `-Denv.java.opts="-XX:+PrintGCDetails -XX:+PrintGCDateStamps`

![[flame_graph.png]]
### 9.3.2 危害
- 延迟越来越高
- 主动拉取的 `Source`：数据积压(`Kafka`)
- 被动接收的 `Source`：`OOM`
- 状态过大
- `Checkpoint` 超时失败
### 9.3.3 原因
- 数据倾斜：有红有绿
- 资源不足：全红且繁忙
- 外部交互：全红不繁忙
### 9.3.4 解决方法
- 增加资源：内存，`CPU`，并行度
- 旁路缓存，异步 `IO`
## 9.4 数据倾斜
- 反压且有红有绿一定是数据倾斜，数据倾斜不一定产生反压
- `WebUI` -> `SubTasks` -> `SubtaskMetrics`
	- 观察多个 `SubTask` 之间的 `Bytes Received`
#### 9.4.1 keyBy 前
- 上游数据分布的不均匀，有的算子处理的数据多
- 需要通过重分区算子进行 `shuffle`
- `Kafka` 按 `Key` 写入时可能造成数据倾斜(不按 `Key` 走黏性分区器不会造成倾斜)，`Upsert Kafka left join` 时指定了 `Key`
#### 9.4.2 keyBy 后
- 直接聚合
	- 预聚合
		- 用状态攒批，按时间或条数输出
	- 加随机数双重聚合
		- <font color='red'>会出现计算错误，实时来一条就会处理一条，最终处理的数量没变</font>
- 开窗聚合
	- 预聚合
		- <font color='red'>聚合后窗口的范围不能确定，会丢失数据的开窗属性</font>
	- 加随机数双重聚合
		- 第一阶段 `key` 拼接随机数开窗聚合
		- 第二阶段按 `key` 以及窗口的 `start` 或 `end` 进行分组聚合
## 9.5 Job 优化
### 9.5.1 使用 DataGenerator 造数据
### 9.5.2 指定算子的 UID
- 指定方式 `.uid().name()`
- `checkpoint` 后如果更改了业务逻辑，添加或者删除了算子，重启后状态和算子的映射在不指定 `UID` 时会无法建立，会导致任务无法重启
### 9.5.3 测量链路延迟
- 监测数据输入、计算和输出的及时性
- 开启方式 
	- `metrics.latency.interval: 30000`
	- `metrics.latency.granularity: operator` 粒度，默认算子
### 9.5.4 开启对象重用
-  设置 `enableObjectReuse()`
- 开启后 Flink 会省略深拷贝的步骤，同一个 Slot 中传输时只发送地址值而非对象
- 适用于
	- 一个对象只会被一个下游 Function 处理
	- 所有下游 Function 都不会改变对象内部的值
### 9.5.5 细粒度滑动窗口优化
- 细粒度滑动窗口，指窗口长度远大于滑动步长，重叠的窗口过多，会产生重复计算
- 可以将其转换为滚动窗口，之后进行在线存储和读时聚合
## 9.6 SQL 优化
- 设置 `ttl`
- 开启 `MiniBatch` 攒批
- ![[minibatch_agg.png]]
	- `configuration.setString("table.exec.mini-batch.enabled", "true")`
	- `configuration.setString("table.exec.mini-batch.allow-latency", "5s")`
	- `configuration.setString("table.exec.mini-batch.size", "20000")`
- LocalGlobal 解决数据倾斜问题，需要和 `MiniBatch` 一起使用
	- `configuration.setString("table.optimizer.agg-phase-strategy", "TWO_PHASE")` 
	- ![[local_global.png]]
- Split Distinct
	- ![[split_distinct.png]]
	- 加随机数打散再聚合
	- 开启方式(结合 `MiniBatch`)
		- `table.optimizer.distinct-agg.split.enabled`
		- `table.optimizer.distinct-agg.split.bucket-num` 打散的 `bucket` 数量，默认 1024

- count distinct 搭配 case when 时可以优化为 filter
```sql
SELECT
 a,
 COUNT(DISTINCT b) AS total_b,
 COUNT(DISTINCT CASE WHEN c IN ('A', 'B') THEN b ELSE NULL END) AS AB_b,
 COUNT(DISTINCT CASE WHEN c IN ('C', 'D') THEN b ELSE NULL END) AS CD_b
FROM T
GROUP BY a

------>

SELECT
 a,
 COUNT(DISTINCT b) AS total_b,
 COUNT(DISTINCT b) FILTER (WHERE c IN ('A', 'B')) AS AB_b,
 COUNT(DISTINCT b) FILTER (WHERE c IN ('C', 'D')) AS CD_b
FROM T
GROUP BY a
```
## 10.1 与 SparkStreaming 的对比
- 本质
	- `Flink` 基于事件触发计算，真正意义上的流处理框架
	- `SparkStreaming` 基于时间触发计算，根据批大小进行处理的微批次处理框架
- 时间语义：`SparkStreaming` 处理时间, `Flink` 有进入，事件，处理三种语义
- 状态编程：
	- `SparkStreaming` 只有一个有状态的算子
		- `updateStateByKey`：当任务挂掉重启时，会将挂掉到当前为止的全部任务一次进行计算，会对内存造成很大负担
	- `Flink` 所有的算子都有状态
- Checkpoint
	- `SparkStreaming` 将所有数据全部保存，包括代码
	- `Flink` 只保存状态数据，可以修改业务逻辑
- 吞吐量
	- `SparkStreaming` 的吞吐量大，因为只有一次网络请求
## 10.2 基础概念
- JobManager
	- 负责资源申请
	- 任务切分
	- 任务管理
	- `Checkpoint` 触发
	- `WebUI`
- TaskManager
	- 提供 `Slot` 执行 `Task`
- 算子链
	- 作用：合并 Task
	- 条件
		- One-to-one 操作：
			- `source, map, flatmap, filter`
			- 不需要重新分区，不需要调整数据顺序
			- 类似 `Spark` 的窄依赖
		- 并行度相同
- Slot 共享组

![[slot_group.svg]]
	- 不同算子的 `Task` 可以共享 `Slot`
	- `Job` 所需 `Slot` 数：各个共享组最大并行度之和
	- 只要属于同一个作业，不同任务的并行子任务可以放到同一个 `slot` 上执行
## 10.3 运行模式、提交方式、提交流程
- 运行模式：Yarn
	- yarn-session: 先创建集群，再提交任务
	- yarn-per-job: 先提交任务，再创建集群，`main` 方法在客户端运行
	- yarn-application：先提交任务，再创建集群，`main` 方法在 `JM` 中运行
	- 创建四种 `Graph` 的位置不同
- 提交方式：
	- 脚本：封装启动任务命令
		- web 端口号
		- 任务名称：默认 `flinkStreamingJob`
	- `StreamPark`
- 提交流程
	- yarn-per-job
		- 本地提交程序：
			- `StreamGraph`
			- 合并算子链得到 `JobGraph`
			- 向 `RM` 提交任务
		- `RM` 找一个 `NM` 启动 `AM`
		- `AM`
			- 启动 `JobManager`：`JobMaster`，`RM`
				- `JobMaster`：将 `JobGraph` 按并行度合并成 `ExecutionGraph`
				- `RM` 申请资源
				- 启动 `TaskManager`
		- `TaskManager`
			- 物理流图：提供 `Slot` 运行 `Task`
	- yarn-application: `StreamGraph` 和 `JobGraph` 在 `JobMaster` 生成
## 10.4 算子

![[process_funcs.svg]]

- `SQL -> Table API -> DataStream -> ProcessFunction`
- 分类
	- Source
	- Transform
		- 基本转换：`map, flatMap, filter`
		- 聚合：`max, maxBy, min, minBy, sum, reduce`
	- 重分区
		- keyBy
		- reblance：全局轮询
		- rescale: 
			- 只会将数据轮询发送到下游并行任务的一部分中
			- 分小团体，小团体内部轮流
		- forward：一对一，上下游并行度相同
		- shuffle：随机
		- broadcast：广播
		- global：全部发往下游第一个分区
	- Sink
- 如何将一个流变为两个流 -- 侧流
	- `getSideOutput()`
	- 侧流输出：`ctx.output(OutputTag x, T t)`
	- 主流输出：`Collector.collect()`
## 10.5 时间语义与窗口操作
- 时间语义：事件、进入、处理
- 事件时间：`Watermark`
	- 本质：流中传输的特殊时间戳，用于衡量事件进行的机制
	- 作用：处理乱序数据
	- 作用机制：延迟关窗
	- 传输：递增(见 4.2)、广播、短板(见 4.5)
	- 生成策略：
		- 周期型：默认 200ms
			- 数据密集时，周期型的好
			- 数据稀疏，产生的水印不会发送
		- 断点式
			- 数据稀疏时，断点式好
			- 数据密集时会产生许多的水印数据
		- 考虑到流中存在数据稀疏和数据密集的情况，周期型更好
- 窗口操作
	- 分类
		- 时间：滚动、滑动、会话
		- 计数：滚动、滑动
	- 核心组件：
		- windowAssigner：
			- `window`
			- `windowAll`
		- 触发器:
			- 凌晨没有数据，按照事件时间开窗，最后一个窗口到第二天才会关闭，及时关闭如何处理
				- 自定义 Trigger
				- 不属于迟到的数据注册两个定时器(事件，处理)
					- `watermark` 不更新，按处理时间触发
					- 一个定时器触发后关闭另一个定时器
			- 开一个 10s 的滚动时间窗口，希望每 2s 输出一次聚合结果
			- 只有 `KeyedStream` 才可以使用定时器
		- 移除器
		- 事件时间
			- 允许迟到：关窗前来一条数据计算一次
			- 侧输出流：当数据所属的所有窗口均已关闭，数据会进侧输出流
		- 窗口聚合函数
			- 增量
				- 效率高
			- 全量 `process WinowFunction`
				- 特殊需求：百分比指标
				- 可以获取窗口信息
			- 结合使用
## 10.6 状态编程与容错
### 10.6.1 状态编程
- 状态：历史数据
	- OperateState
		- `List`
		- `UnionList`
		- `BroadcastState`
	- KeyedState
		- `Value`
		- `List`
		- `Map`
		- `Reduce`
		- `Aggregate`
	- 使用在 `RichFunction` 中，可以设置 TTL
### 10.6.2 容错
- Checkpoint / SavePoint
	- Barrier 对齐
		- 精确一次
			- 有 `Barrier` 到达后
				- `Barrier` 已到达的 `Task` 在收到数据时进行缓存数据
				- `Barrier` 未到达的 `Task` 来数据时，正常计算输出
			- 所有的 `Barrier` 均到达后将计算结果(状态)保存
			- 将缓存的数据依次处理并输出
		- 至少一次
			- 有 `Barrier` 到达后
				- `Barrier` 已到达的 `Task` 在收到数据时继续计算，不缓存
				- 挂掉之后会重复读
				- `Barrier` 未到达的 `Task` 来数据时，正常计算输出
			- 所有的 `Barrier` 均到达后将状态保存
	- 非 Barrier 对齐
		- 有 `Barrier` 到达后，会将当前计算结果保存，`ck` 为未完成状态
			- `Barrier` 已到达的 `Task` 收到数据时，正常计算输出
			- `Barrier` 未到达的 `Task` 来数据时，正常计算输出，同时将数据本身进行缓存
		- 所有 `Barrier` 都到达后，`ck` 标记完成
		- 缺点：会造成状态过大
	- 端到端精准一次
		- Source：可重复读取数据
		- Flink：精准一次 `checkpoint`
		- Sink：
			- 幂等
			- 事务 2PC
## 10.7 FlinkSQL
- 事件时间，处理时间的提取
	- 建表时创建
		- `pc as proctime()`
		- `et as to_timestamp_ltz(ts, 3) (ts, 3) -> ms (ts, 0) -> s`
	- 流转表
- 窗口
	- TVF: 
		- 优化
		- 多维分析函数
		- 累计窗口
	- GroupWindow：会话窗口
- 自定义函数
	- UDF
	- UDAF
	- UDTF
	- UDTAF: 只用于 `TableAPI`
- Join
	- 常规 Join：内连接，外连接
		- 左外连接：TTL = 10s
			- 左表先来数据，10s 内右表来数据
				- \[+I]    左    null
				- \[-D]    左   null
				- \[+I]    左右
			- 左表先来数据，10s 后右表来数据
				- \[+I]    左    null
			- 左表先来数据，10s 内右表来数据，之后保持间隔 10s 内来数据
				- \[+I]    左    null
				- \[-D]    左    null
				- \[+I]    左右
					- 左表 `ttl` 的类型是 `OnReadAndWrite`, 读一次后 `ttl` 的时间会刷新
				- 后续会保持\[+I] 左右
			- 左表先来数据，10s 内右表来数据，之后超过 10s 来数据
				- \[+I]    左    null
				- \[-D]    左    null
				- \[+I]    左右
				- 之后不打印任何数据
			- 右表先来数据，10s 内左表来数据
				- \[+I]    左右
			- 右表先来数据，10s 后左表来数据
				- \[+I]    左    null
			- 右表先来数据，10s 内左表来数据，之后保持间隔 10s 内来数据
				- 10s 内：\[+I]    左右
				- 10s 后：\[+I]    左    null
					- 右表 `ttl` 类型是 `OnCreateAndWrite`，读取并不会刷新 `ttl` 时间
			- 右表先来数据，10s 内左表来数据，之后超过 10s 来数据
				- 同上
	- 时序 Join：事件、处理
	- LookupJoin：时间语义下特殊的时序 `Join`
		- Cache: 维表数据不更新时使用
	- IntervalJoin